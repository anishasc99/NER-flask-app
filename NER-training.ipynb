{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<Hi, Ramesh, ,, This, is, with, reference, to, our, call, in, the, morning, .>\n",
      "TokenList<As, discussed, ,, I, would, be, requiring, Melanalin, -, 109B, .>\n",
      "TokenList<The, quantity, required, will, be, 100, ton, .>\n",
      "TokenList<Hope, you, will, be, able, to, fulfill, this, order, .>\n",
      "TokenList<Thanks, and, regards, ,, Suresh>\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "from conllu import parse_incr\n",
    "\n",
    "\n",
    "data = open(\"text-1.conllu\", \"r\", encoding=\"utf-8\")\n",
    "sentences = []\n",
    "for tokenlist in parse_incr(data):\n",
    "    print(tokenlist)\n",
    "    sentences.append(tokenlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenList<As, discussed, ,, I, would, be, requiring, Melanalin, -, 109B, .>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentences[1]\n",
    "sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 8,\n",
       " 'form': 'Melanalin',\n",
       " 'lemma': '_',\n",
       " 'upos': '_',\n",
       " 'xpos': None,\n",
       " 'feats': None,\n",
       " 'head': None,\n",
       " 'deprel': '_',\n",
       " 'deps': None,\n",
       " 'misc': {'SpaceAfter': 'No'}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = sentence[7]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'As discussed, I would be requiring Melanalin-109B.'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = parse_incr(data, metadata_parsers={\"tagset\": lambda key, value: (key, value.split(\"|\"))})\n",
    "for sentence in sentences:\n",
    "    print(sentence.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, pprint\n",
    "\n",
    "\n",
    "class convert_conll2spacy(object):\n",
    "\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "\n",
    "    def convert(self):\n",
    "        #print(\"Start Conversion\")\n",
    "        with open(self.file, 'r') as devset:\n",
    "            content = csv.reader(devset, delimiter=' ', skipinitialspace=True, quotechar=None)\n",
    "\n",
    "            text_as_list = []\n",
    "            sentence_as_list = []\n",
    "            entities = []\n",
    "            sentences_as_plain_text = \"\"\n",
    "            i = 0\n",
    "            tokenized_list = []\n",
    "\n",
    "            for row in content:\n",
    "                #print(len(row))\n",
    "                if len(row) == 0:\n",
    "                    tokenized_list.append(\" \")\n",
    "                else:\n",
    "                    tokenized_list.append(row[0])\n",
    "                if len(row) == 2:\n",
    "                    if 'B-PROD' in row[1]:\n",
    "                        start = i\n",
    "                        end = i+len(row[0])\n",
    "                        entities.append((start, end, 'B-PROD'))\n",
    "                    if 'I-PROD' in row[1]:\n",
    "                        start = i\n",
    "                        end = i+len(row[0])\n",
    "                        entities.append((start, end, 'I-PROD'))\n",
    "                    if 'B-QUAN' in row[1]:\n",
    "                        start = i\n",
    "                        end = i+len(row[0])\n",
    "                        entities.append((start, end, 'B-QUAN'))\n",
    "                    if 'I-QUAN' in row[1]:\n",
    "                        start = i\n",
    "                        end = i+len(row[0])\n",
    "                        entities.append((start, end, 'I-QUAN'))\n",
    "                    \n",
    "                    \n",
    "                    sentence_as_list.append(row[0])\n",
    "                    i += len(row[0])+1\n",
    "\n",
    "                elif len(row) == 0:\n",
    "                    i = 0\n",
    "                    sentence = \" \".join(sentence_as_list)\n",
    "                    sentences_as_plain_text += sentence\n",
    "                    add_sent_ne_to_list = (sentence, entities)\n",
    "                    text_as_list.append(add_sent_ne_to_list)\n",
    "                    sentence_as_list = []\n",
    "                    entities = []\n",
    "\n",
    "        #pprint.pprint(text_as_list)\n",
    "        #print(\"Conversion done!\")\n",
    "        return text_as_list, sentences_as_plain_text, tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv = convert_conll2spacy(\"./annotated emails/email_1.conll\")\n",
    "text_as_list, sentences_as_plain_text, tokenized_list = Cv.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello , Manager I am looking for differents products from you .', []), ('Their quantity and products Names are 8,621 x Acetylene , 4,752 x Grignard , 4,175 x Ethoxide , 5,247 x Methylene , 4,681 x Benzolide .', [(38, 43, 'B-QUAN'), (46, 55, 'B-PROD'), (58, 63, 'B-QUAN'), (66, 74, 'B-PROD'), (77, 82, 'B-QUAN'), (85, 93, 'B-PROD'), (96, 101, 'B-QUAN'), (104, 113, 'B-PROD'), (116, 121, 'B-QUAN'), (124, 133, 'B-PROD')]), ('Very truly yours , Mayur', [])]\n"
     ]
    }
   ],
   "source": [
    "print (text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2json(text_as_list):\n",
    "    TRAIN_DATA = []\n",
    "    for text in text_as_list:\n",
    "            mydict = {}\n",
    "        #if len(text[1]) !=0:\n",
    "            sentence = text[0]\n",
    "            entities = text[1]\n",
    "            mydict[\"entities\"] = entities\n",
    "            temp = []\n",
    "            temp.append(sentence)\n",
    "            temp.append(mydict)\n",
    "            TRAIN_DATA.append(tuple(temp))\n",
    "    return TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello , Manager I am looking for differents products from you .',\n",
       "  {'entities': []}),\n",
       " ('Their quantity and products Names are 8,621 x Acetylene , 4,752 x Grignard , 4,175 x Ethoxide , 5,247 x Methylene , 4,681 x Benzolide .',\n",
       "  {'entities': [(38, 43, 'B-QUAN'),\n",
       "    (46, 55, 'B-PROD'),\n",
       "    (58, 63, 'B-QUAN'),\n",
       "    (66, 74, 'B-PROD'),\n",
       "    (77, 82, 'B-QUAN'),\n",
       "    (85, 93, 'B-PROD'),\n",
       "    (96, 101, 'B-QUAN'),\n",
       "    (104, 113, 'B-PROD'),\n",
       "    (116, 121, 'B-QUAN'),\n",
       "    (124, 133, 'B-PROD')]}),\n",
       " ('Very truly yours , Mayur', {'entities': []})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert2json(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(model=None, output_dir=None, n_iter=100):\n",
    "    TRAIN_DATA = []\n",
    "    for i in range(1,22):\n",
    "        file_name = \"./annotated emails/email_\"+str(i)+\".conll\"\n",
    "        Cv = convert_conll2spacy(file_name)\n",
    "        text_as_list, sentences_as_plain_text, tokenized_list = Cv.convert()\n",
    "        TRAIN = convert2json(text_as_list)\n",
    "        TRAIN_DATA.extend(TRAIN)\n",
    "    print(len(TRAIN_DATA))\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        #print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "       # print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    print(output_dir)\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "           # print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            #print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "        #test_text = 'We will required 50 tons of Methylin'\n",
    "        #doc = nlp(test_text)\n",
    "        #print(\"Entities in '%s'\" % test_text)\n",
    "        #for ent in doc.ents:\n",
    "         #   print(ent.label_, ent.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(s):\n",
    "    nlp = spacy.load(output_dir)\n",
    "    doc = nlp(s)\n",
    "    print(\"Entities in '%s'\" % s)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M433IA\\anaconda3\\lib\\site-packages\\spacy\\language.py:635: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  proc.begin_training(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 468.7806473376113}\n",
      "Losses {'ner': 191.97906185360625}\n",
      "Losses {'ner': 144.03741665799149}\n",
      "Losses {'ner': 84.48628679424885}\n",
      "Losses {'ner': 64.98362858186886}\n",
      "Losses {'ner': 40.8949210172205}\n",
      "Losses {'ner': 24.088203574147425}\n",
      "Losses {'ner': 23.80790276051456}\n",
      "Losses {'ner': 20.470228893705997}\n",
      "Losses {'ner': 9.36006137107237}\n",
      "Losses {'ner': 11.405717174735658}\n",
      "Losses {'ner': 6.510289813501757}\n",
      "Losses {'ner': 4.417241689669931}\n",
      "Losses {'ner': 8.744494100379768}\n",
      "Losses {'ner': 10.032167074484224}\n",
      "Losses {'ner': 5.0593042704165505}\n",
      "Losses {'ner': 11.229817473524681}\n",
      "Losses {'ner': 6.57498925865661}\n",
      "Losses {'ner': 4.211313066415103}\n",
      "Losses {'ner': 2.909562610733797}\n",
      "Losses {'ner': 4.740391844110788}\n",
      "Losses {'ner': 5.9740649112128885}\n",
      "Losses {'ner': 9.410325130419821}\n",
      "Losses {'ner': 4.091918190700192}\n",
      "Losses {'ner': 7.555363493755213}\n",
      "Losses {'ner': 4.316653373438908}\n",
      "Losses {'ner': 2.408570138004756}\n",
      "Losses {'ner': 9.421942165590224}\n",
      "Losses {'ner': 1.2977967971710962}\n",
      "Losses {'ner': 0.7160900056051485}\n",
      "Losses {'ner': 0.44294902153648763}\n",
      "Losses {'ner': 2.9824981264699764}\n",
      "Losses {'ner': 0.0048684242130578375}\n",
      "Losses {'ner': 1.9742021549130373}\n",
      "Losses {'ner': 1.5798494462996246}\n",
      "Losses {'ner': 2.2264374894685472}\n",
      "Losses {'ner': 1.9790465403931834}\n",
      "Losses {'ner': 1.7853999141918009}\n",
      "Losses {'ner': 0.012796212135448943}\n",
      "Losses {'ner': 6.721986316277768}\n",
      "Losses {'ner': 6.916052576480048e-05}\n",
      "Losses {'ner': 0.004209608750790204}\n",
      "Losses {'ner': 5.911338446745741}\n",
      "Losses {'ner': 3.301412648660313}\n",
      "Losses {'ner': 1.8618803148194063}\n",
      "Losses {'ner': 2.1494078504810994}\n",
      "Losses {'ner': 0.01324256294284973}\n",
      "Losses {'ner': 0.25663719850626465}\n",
      "Losses {'ner': 1.590570318736716}\n",
      "Losses {'ner': 2.0094715649829196}\n",
      "Losses {'ner': 0.3350716361006893}\n",
      "Losses {'ner': 0.006182747968488868}\n",
      "Losses {'ner': 0.1236700329077329}\n",
      "Losses {'ner': 1.8941812555322206}\n",
      "Losses {'ner': 3.053078162367605}\n",
      "Losses {'ner': 2.4041582973582964e-05}\n",
      "Losses {'ner': 0.0015318629743255028}\n",
      "Losses {'ner': 1.694242889705711}\n",
      "Losses {'ner': 1.1776702159634302e-07}\n",
      "Losses {'ner': 0.00018479749065308674}\n",
      "Losses {'ner': 0.013186146542306518}\n",
      "Losses {'ner': 0.00012385072501504607}\n",
      "Losses {'ner': 1.1390701470613067}\n",
      "Losses {'ner': 1.8304117635960135}\n",
      "Losses {'ner': 0.10872641971179117}\n",
      "Losses {'ner': 0.0003859108356569551}\n",
      "Losses {'ner': 7.125620088553453e-06}\n",
      "Losses {'ner': 1.4884469511746754}\n",
      "Losses {'ner': 2.352726229579858e-10}\n",
      "Losses {'ner': 6.294861138932185e-07}\n",
      "Losses {'ner': 0.10412139122417274}\n",
      "Losses {'ner': 2.2599828278407414e-05}\n",
      "Losses {'ner': 4.599754262722673e-11}\n",
      "Losses {'ner': 0.8929581980910024}\n",
      "Losses {'ner': 0.013604414724390607}\n",
      "Losses {'ner': 1.31145233912968}\n",
      "Losses {'ner': 0.05418279863333676}\n",
      "Losses {'ner': 2.0335686513269167}\n",
      "Losses {'ner': 0.0001997487454367838}\n",
      "Losses {'ner': 2.752416196683669}\n",
      "Losses {'ner': 0.004577943326366361}\n",
      "Losses {'ner': 0.16766493229682153}\n",
      "Losses {'ner': 1.2177982188206154e-08}\n",
      "Losses {'ner': 1.9921058087499093}\n",
      "Losses {'ner': 3.145001654054052}\n",
      "Losses {'ner': 2.3262820887368143}\n",
      "Losses {'ner': 0.0002719806594204688}\n",
      "Losses {'ner': 1.737697017098252}\n",
      "Losses {'ner': 2.042593949704533}\n",
      "Losses {'ner': 0.5442921864514292}\n",
      "Losses {'ner': 2.0441814857155713}\n",
      "Losses {'ner': 1.6694617920408883e-10}\n",
      "Losses {'ner': 1.9990569766359874}\n",
      "Losses {'ner': 3.0671169377276413}\n",
      "Losses {'ner': 5.466109165301012e-09}\n",
      "Losses {'ner': 1.9984108194050978}\n",
      "Losses {'ner': 1.5893413654347603}\n",
      "Losses {'ner': 0.16234491641523036}\n",
      "Losses {'ner': 0.020442523913991286}\n",
      "Losses {'ner': 1.4692524733974461}\n",
      "./nlp\n",
      "Saved model to nlp\n",
      "Loading from nlp\n",
      "Enter any text: Hi Tarun, need  100 kg of Epoxy. Hope you can deliver on time. Thanks and regards, Anisha\n",
      "Entities in 'Hi Tarun, need  100 kg of Epoxy. Hope you can deliver on time. Thanks and regards, Anisha'\n",
      "B-QUAN 100\n",
      "B-PROD Epoxy\n",
      "B-PROD Anisha\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   # plac.call(main)\n",
    "    output_dir=\"./nlp\"\n",
    "    training(output_dir = './nlp')\n",
    "    s = input(\"Enter any text: \")\n",
    "    testing(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in ' Hi Tarun, need  100, 400 and 300 tons of Epoxy, Boxofine and Ramflyn respectively. Hope you can deliver on time. Thanks and regards, Anisha'\n",
      "B-QUAN 100\n",
      "B-QUAN 400\n",
      "B-QUAN 300\n",
      "B-PROD Epoxy\n",
      "B-PROD Boxofine\n",
      "B-PROD Ramflyn\n"
     ]
    }
   ],
   "source": [
    "testing(\" Hi Tarun, need  100, 400 and 300 tons of Epoxy, Boxofine and Ramflyn respectively. Hope you can deliver on time. Thanks and regards, Anisha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter mail: Hi Tarun, need  100, 400 and 300 tons of Epoxy, Boxofine and Ramflyn respectively. Hope you can deliver on time. Thanks and regards, Anisha\n",
      "Entities in 'Hi Tarun, need  100, 400 and 300 tons of Epoxy, Boxofine and Ramflyn respectively. Hope you can deliver on time. Thanks and regards, Anisha'\n",
      "B-QUAN 100\n",
      "B-QUAN 400\n",
      "B-QUAN 300\n",
      "B-PROD Epoxy\n",
      "B-PROD Boxofine\n",
      "B-PROD Ramflyn\n"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter mail: \")\n",
    "testing(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
